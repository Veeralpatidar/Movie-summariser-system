{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# End-to-End Demo: Fine-Tuning BERT for Extractive Summarization\n\nThis notebook provides a complete walkthrough of fine-tuning a pre-trained BERT model for extractive summarization. It assumes you have already run `preprocessing.ipynb` to generate a dataset of scripts and their corresponding sentence labels.\n\n**Workflow:**\n1.  **Setup**: Install dependencies and define necessary classes and functions from the training script.\n2.  **Data Simulation**: We'll create a small, representative dataset on the fly to make this notebook self-contained. This simulates the output of the `preprocessing.ipynb`.\n3.  **Configuration**: Set up training parameters like model name, learning rate, and epochs.\n4.  **Training**: Run the complete training and validation loop.\n5.  **Inference**: Load the best-performing model and use it to summarize a new, unseen movie script snippet.","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch tqdm -q","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport re\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AdamW\nfrom tqdm.notebook import tqdm\n\n# --- Code from train_bert_summarizer.py ---\n\ndef sentence_split(text: str) -> List[str]:\n    \"\"\"Naive sentence splitter used for scripts and dialogues.\"\"\"\n    text = text.replace('\\r', '\\n')\n    sents = re.split(r'(?<=[.!?\\n])\\s+', text)\n    sents = [s.strip() for s in sents if s.strip()]\n    return sents\n\nclass BertSummarizer(nn.Module):\n    \"\"\"\n    A BERT-based model for extractive summarization.\n    \"\"\"\n    def __init__(self, model_name: str):\n        super(BertSummarizer, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.scorer = nn.Linear(self.bert.config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask, cls_indices):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = outputs.last_hidden_state\n        batch_size, _, _ = last_hidden_state.shape\n        batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, cls_indices.size(1))\n        batch_indices = batch_indices.to(last_hidden_state.device)\n        sent_reps = last_hidden_state[batch_indices, cls_indices]\n        scores = self.scorer(sent_reps).squeeze(-1)\n        return scores\n\nclass SummarizationDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for loading scripts and their summarization labels.\n    \"\"\"\n    def __init__(self, data_dir: str, tokenizer, max_len: int = 512):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.txt')]\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        filepath = self.files[idx]\n        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n            text = f.read()\n\n        sentences = sentence_split(text)\n        label_path = filepath.replace('.txt', '.labels.json')\n        try:\n            with open(label_path, 'r') as f:\n                label_data = json.load(f)\n                important_indices = set(label_data.get('important_sentence_indices', []))\n        except FileNotFoundError:\n            important_indices = set()\n            \n        input_tokens = [self.tokenizer.cls_token]\n        cls_indices = []\n        labels = []\n\n        for i, sentence in enumerate(sentences):\n            cls_indices.append(len(input_tokens))\n            labels.append(1.0 if i in important_indices else 0.0)\n            sent_tokens = self.tokenizer.tokenize(sentence)\n            input_tokens.extend(sent_tokens)\n            \n            if len(input_tokens) >= self.max_len - 1:\n                input_tokens = input_tokens[:self.max_len - 1]\n                while cls_indices and cls_indices[-1] >= len(input_tokens):\n                    cls_indices.pop()\n                    labels.pop()\n                break\n        \n        input_tokens.append(self.tokenizer.sep_token)\n        input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            'cls_indices': torch.tensor(cls_indices, dtype=torch.long),\n            'labels': torch.tensor(labels, dtype=torch.float)\n        }\n\ndef collate_fn(batch):\n    input_ids = torch.nn.utils.rnn.pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=0)\n    attention_mask = torch.nn.utils.rnn.pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n    labels = torch.nn.utils.rnn.pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-1.0)\n    cls_indices = torch.nn.utils.rnn.pad_sequence([item['cls_indices'] for item in batch], batch_first=True, padding_value=0)\n    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'cls_indices': cls_indices, 'labels': labels}\n\ndef train_epoch(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        cls_indices = batch['cls_indices'].to(device)\n        labels = batch['labels'].to(device)\n        scores = model(input_ids, attention_mask, cls_indices)\n        label_mask = (labels != -1.0).float()\n        max_sents = label_mask.sum(dim=1).max().int()\n        scores = scores[:, :max_sents]\n        labels = labels[:, :max_sents]\n        label_mask = label_mask[:, :max_sents]\n        loss = nn.BCEWithLogitsLoss(weight=label_mask)(scores, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef validate_epoch(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Validating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            cls_indices = batch['cls_indices'].to(device)\n            labels = batch['labels'].to(device)\n            scores = model(input_ids, attention_mask, cls_indices)\n            label_mask = (labels != -1.0).float()\n            max_sents = label_mask.sum(dim=1).max().int()\n            scores = scores[:, :max_sents]\n            labels = labels[:, :max_sents]\n            label_mask = label_mask[:, :max_sents]\n            loss = nn.BCEWithLogitsLoss(weight=label_mask)(scores, labels)\n            total_loss += loss.item()\n    return total_loss / len(dataloader)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Data Simulation\n\nYour `preprocessing.ipynb` notebook generates a directory of labeled data. For this demo, we'll create a temporary directory (`./temp_data`) and populate it with a few examples that mimic that structure. This allows the notebook to run from start to finish without external dependencies.","metadata":{}},{"cell_type":"code","source":"DATA_DIR = './temp_data'\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Sample 1: A short dialogue\nscript1_text = \"Amanda: Did you get the files? John: Yes, I have them right here. Amanda: Great. Let's get to work then. We don't have much time.\"\nscript1_labels = {\"important_sentence_indices\": [1, 3]}\n\nwith open(os.path.join(DATA_DIR, 'script1.txt'), 'w') as f:\n    f.write(script1_text)\nwith open(os.path.join(DATA_DIR, 'script1.labels.json'), 'w') as f:\n    json.dump(script1_labels, f)\n\n# Sample 2: A longer scene description\nscript2_text = \"The sun sets over the city. A lone figure stands on a rooftop, looking down at the bustling streets. Sirens wail in the distance. The figure pulls up their hood, their face obscured by shadow. A decision has to be made tonight.\"\nscript2_labels = {\"important_sentence_indices\": [1, 4]}\n\nwith open(os.path.join(DATA_DIR, 'script2.txt'), 'w') as f:\n    f.write(script2_text)\nwith open(os.path.join(DATA_DIR, 'script2.labels.json'), 'w') as f:\n    json.dump(script2_labels, f)\n    \n# Create 18 more dummy files for a slightly larger dataset\nfor i in range(3, 21):\n    text = f\"This is sentence one. This is the crucial second sentence. And this is the final sentence of script {i}.\"\n    labels = {\"important_sentence_indices\": [1]}\n    with open(os.path.join(DATA_DIR, f'script{i}.txt'), 'w') as f:\n        f.write(text)\n    with open(os.path.join(DATA_DIR, f'script{i}.labels.json'), 'w') as f:\n        json.dump(labels, f)\n\nprint(f\"Created {len(os.listdir(DATA_DIR))//2} dummy script samples in '{DATA_DIR}'\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Configuration & Data Loading\n\nHere, we'll define our training configuration and prepare the datasets and dataloaders for training.","metadata":{}},{"cell_type":"code","source":"# --- CONFIGURATION ---\nMODEL_NAME = 'distilbert-base-uncased' # Using a smaller model for a quick demo\nEPOCHS = 3\nBATCH_SIZE = 4\nLR = 2e-5\nVAL_SPLIT = 0.2\nSAVE_PATH = './bert_summarizer_model'\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# --- DATA PREPARATION ---\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nfull_dataset = SummarizationDataset(DATA_DIR, tokenizer)\nval_size = int(len(full_dataset) * VAL_SPLIT)\ntrain_size = len(full_dataset) - val_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n\nprint(f\"Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples.\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Model Training\n\nNow we'll initialize the model and optimizer and run the main training loop. The model that performs best on the validation set will be saved to the `SAVE_PATH`.","metadata":{}},{"cell_type":"code","source":"model = BertSummarizer(MODEL_NAME).to(device)\noptimizer = AdamW(model.parameters(), lr=LR)\n\nbest_val_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n    train_loss = train_epoch(model, train_loader, optimizer, device)\n    val_loss = validate_epoch(model, val_loader, device)\n    \n    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        print(\"New best validation loss. Saving model...\")\n        os.makedirs(SAVE_PATH, exist_ok=True)\n        model.bert.save_pretrained(SAVE_PATH)\n        torch.save(model.scorer.state_dict(), os.path.join(SAVE_PATH, 'scorer.pt'))\n        tokenizer.save_pretrained(SAVE_PATH)\n\nprint(\"\\nTraining complete!\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")\nprint(f\"Model saved to {SAVE_PATH}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Inference\n\nWith the model trained, let's test its summarization capabilities. We'll define an `infer` function, load our best model from disk, and feed it a new script snippet it has never seen before.","metadata":{}},{"cell_type":"code","source":"def infer_summary(script_text: str, model: BertSummarizer, tokenizer, device, top_k: int = 2):\n    model.eval()\n    sentences = sentence_split(script_text)\n    \n    # Prepare input\n    input_tokens = [tokenizer.cls_token]\n    cls_indices = []\n    for sentence in sentences:\n        cls_indices.append(len(input_tokens))\n        sent_tokens = tokenizer.tokenize(sentence)\n        input_tokens.extend(sent_tokens)\n    input_tokens.append(tokenizer.sep_token)\n\n    input_ids = torch.tensor(tokenizer.convert_tokens_to_ids(input_tokens), dtype=torch.long).unsqueeze(0).to(device)\n    attention_mask = torch.ones_like(input_ids).to(device)\n    cls_indices = torch.tensor(cls_indices, dtype=torch.long).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        scores = model(input_ids, attention_mask, cls_indices).squeeze(0)\n        sigmoid_scores = torch.sigmoid(scores)\n\n    # Select top-k sentences\n    top_indices = torch.argsort(sigmoid_scores, descending=True)[:top_k]\n    sorted_indices = sorted(top_indices.tolist())\n    \n    summary = \" \".join([sentences[i] for i in sorted_indices])\n    return summary\n\n# --- Load the fine-tuned model ---\nprint(f\"Loading model from {SAVE_PATH}\")\ninference_tokenizer = AutoTokenizer.from_pretrained(SAVE_PATH)\ninference_model = BertSummarizer(SAVE_PATH).to(device) # AutoModel loads from the directory\ninference_model.scorer.load_state_dict(torch.load(os.path.join(SAVE_PATH, 'scorer.pt')))\n\n# --- Run Inference ---\ntest_script = \"\"\"\nINT. WAREHOUSE - NIGHT\n\nDETECTIVE MILLER shines his flashlight across dusty crates. This place hasn't been touched in years. His partner, DETECTIVE SANTIAGO, kicks at a loose floorboard. It's a dead end. \n\nSANTIAGO\nNothing. We've been had.\n\nMILLER\nMaybe not. Look at this.\n\nMiller points his light to a small, almost invisible symbol carved into a crate. The key symbol. Santiago's eyes widen. This changes everything.\n\"\"\"\n\ngenerated_summary = infer_summary(test_script, inference_model, inference_tokenizer, device, top_k=3)\n\nprint(\"--- ORIGINAL SCRIPT SNIPPET ---\")\nprint(test_script)\nprint(\"\\n--- GENERATED SUMMARY ---\")\nprint(generated_summary)","metadata":{},"outputs":[],"execution_count":null}]}